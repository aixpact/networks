{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning on graphs\n",
    "\n",
    "- prediction based on graph metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.2** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-social-network-analysis/resources/yPcBs) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep -i netw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1 - Random Graph Identification\n",
    "\n",
    "For the first part of this assignment you will analyze randomly generated graphs and determine which algorithm created them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1_Graphs = pickle.load(open('../_data/A4_graphs.dms','rb'))\n",
    "P1_Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "`P1_Graphs` is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:\n",
    "* Preferential Attachment (`'PA'`)\n",
    "* Small World with low probability of rewiring (`'SW_L'`)\n",
    "* Small World with high probability of rewiring (`'SW_H'`)\n",
    "\n",
    "Anaylze each of the 5 graphs and determine which of the three algorithms generated the graph.\n",
    "\n",
    "*The `graph_identification` function should return a list of length 5 where each element in the list is either `'PA'`, `'SW_L'`, or `'SW_H'`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Real World:__\n",
    "\n",
    " - degrees are distributed as __Power Law__ (log/log degree distribution is straight line)\n",
    " - shortest path < 7,5\n",
    " - clustering < 0.1\n",
    "\n",
    "\n",
    "__Small world:__\n",
    "\n",
    " - degrees are NOT distributed as __Power Law__ \n",
    " - More nodes => \n",
    "   - higher average shortest path\n",
    "   - lower average clustering\n",
    " - Higher rewiring p: lower clustering and lower shortest path\n",
    " \n",
    "- __SW_L: Lattice - Small World:__\n",
    " - higher shortest paths > 7.5, higher clustering > 0.1\n",
    " - max clustering = 0.1\n",
    " - max shortest path = 7,5\n",
    "\n",
    "- __SW_H: Small world - Random:__ \n",
    " - lower shortest paths < 7.5, lower clustering > 0.02\n",
    " - max clustering = 0.02\n",
    " - max shortest path = 4,5\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_type(G):\n",
    "    \"\"\"Return graph type.\"\"\"\n",
    "    degrees = Counter(dict(nx.degree(G)).values())\n",
    "    shortest_path = nx.average_shortest_path_length(G) \n",
    "    clustering = nx.average_clustering(G)\n",
    "    most_common = degrees.most_common(5)\n",
    "    # distribution is exponential - monotonically decreasing\n",
    "    if all(i[0] <= j[0] for i, j in zip(most_common, most_common[1:])):\n",
    "        graph_type = 'PA'\n",
    "    elif (shortest_path < 7.5) | (clustering < 0.25):\n",
    "        graph_type = 'SW_H'\n",
    "    else:\n",
    "        graph_type = 'SW_L'\n",
    "        \n",
    "    template = 'Graph type: {} \\nShortest path length: {:.2f}, Clustering: {:.2f}'.format(\n",
    "        graph_type, shortest_path, clustering)\n",
    "    return template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barabasi-Albert graph \n",
    "\n",
    "- Preferential attachment mode\n",
    "\n",
    "https://networkx.github.io/documentation/networkx-1.9/reference/generators.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of degrees\n",
    "# Number of edges to attach from a new node to existing nodes\n",
    "from collections import Counter\n",
    "for edges in [1, 2, 5, 20]:\n",
    "    G = nx.barabasi_albert_graph(100, edges)\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "    ttl = nx.number_of_nodes(G)\n",
    "    degrees = Counter(dict(nx.degree(G)).values()).keys()\n",
    "    fraction = np.array(list(Counter(dict(nx.degree(G)).values()).values()))/ttl\n",
    "    _ = ax1.bar(degrees, fraction)\n",
    "    _ = ax1.set_xlabel('Degree')\n",
    "    _ = ax1.set_ylabel('Fraction of Nodes')\n",
    "    _ = ax1.spines['top'].set_visible(False)\n",
    "    _ = ax1.spines['right'].set_visible(False)\n",
    "    _ = nx.draw_networkx(G, node_size=.7, edge_with=.2, with_labels=False, alpha=.5, ax=ax2)\n",
    "    _ = plt.suptitle('Barabasi-Albert graph, # connecting edges:{}\\n{}'.format(edges, graph_type(G)))\n",
    "    _ = ax2.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play ground and sanity check graph data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(nx.degree(G))                            # edges per node\n",
    "# Counter(dict(nx.degree(G)))                   # edges per node\n",
    "Counter(dict(nx.degree(G)).values())            # distribution of # edges as dict\n",
    "Counter(dict(nx.degree(G)).values()).items()    # same as tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watts-Strogatz graph\n",
    "\n",
    "- small-world model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watts-Strogatz small-world graph.\n",
    "# The number of nodes\n",
    "# Each node is connected to k nearest neighbors in ring topology\n",
    "# The probability of rewiring each edge\n",
    "\n",
    "from collections import Counter\n",
    "for knn in [2, 5, 10]:\n",
    "    for proba in [.1, .2, .4, .8, 1.]:\n",
    "        try:\n",
    "            G = nx.connected_watts_strogatz_graph(n, knn, proba, tries=1000, seed=0)\n",
    "            fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "            ttl = nx.number_of_nodes(G)\n",
    "            degrees = Counter(dict(nx.degree(G)).values()).keys()\n",
    "            fraction = np.array(list(Counter(dict(nx.degree(G)).values()).values()))/ttl\n",
    "            _ = ax1.bar(degrees, fraction)\n",
    "            _ = ax1.set_xlabel('Degree')\n",
    "            _ = ax1.set_ylabel('Fraction of Nodes')\n",
    "            _ = ax1.spines['top'].set_visible(False)\n",
    "            _ = ax1.spines['right'].set_visible(False)\n",
    "            _ = nx.draw_networkx(G, node_size=.7, edge_with=.2, with_labels=False, alpha=.5, ax=ax2)\n",
    "            _ = plt.suptitle('Connected Watts-Strogatz graph \\nnodes: {}, k-nn: {}, proba: {}\\n{}'.format(\n",
    "                n, knn, proba, graph_type(G)))\n",
    "            _ = ax2.axis('off')\n",
    "            plt.show();\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holme and Kim algorithm for growing graphs with powerlaw\n",
    "# The number of nodes\n",
    "# The number of random edges to add for each new node\n",
    "# Probability of adding a triangle after adding a random edge\n",
    "\n",
    "n=100\n",
    "from collections import Counter\n",
    "for re in [2, 4, 8]:\n",
    "    for proba in [.1, .2, .4, .8, 1.]:\n",
    "        try:\n",
    "            G = nx.powerlaw_cluster_graph(n, re, proba, seed=0) #(n, knn, proba, tries=1000, seed=0)\n",
    "            fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
    "            ttl = nx.number_of_nodes(G)\n",
    "            degrees = Counter(dict(nx.degree(G)).values()).keys()\n",
    "            fraction = np.array(list(Counter(dict(nx.degree(G)).values()).values()))/ttl\n",
    "            _ = ax1.bar(degrees, fraction)\n",
    "            _ = ax1.set_xlabel('Degree')\n",
    "            _ = ax1.set_ylabel('Fraction of Nodes')\n",
    "            _ = ax1.spines['top'].set_visible(False)\n",
    "            _ = ax1.spines['right'].set_visible(False)\n",
    "            _ = nx.draw_networkx(G, node_size=.7, edge_with=.2, with_labels=False, alpha=.5, ax=ax2)\n",
    "            _ = plt.suptitle('Holme & Kim growing graph \\nnodes: {}, k-nn: {}, proba: {}\\n{}'.format(\n",
    "                n, re, proba, graph_type(G)))\n",
    "            _ = ax2.axis('off')\n",
    "            plt.show();\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 - Company Emails\n",
    "\n",
    "For the second part of this assignment you will be workking with a company's email network where each node corresponds to a person at the company, and each edge indicates that at least one email has been sent between two people.\n",
    "\n",
    "The network also contains the node attributes `Department` and `ManagementSalary`.\n",
    "\n",
    "`Department` indicates the department in the company which the person belongs to, and `ManagementSalary` indicates whether that person is receiving a management position salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert networkx 1.X pickle-file to 2.X\n",
    "\n",
    "The Pickle protocol does not store class methods, only the data. So if you write a pickle file with v1 you should not expect to read it into a v2 Graph. If this happens to you, read it in with v1 installed and write a file with the node and edge information. You can read that into a config with v2 installed and then add those nodes and edges to a fresh graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Open different environment:\n",
    "!pip list | grep -i network\n",
    "!pip install networkx==1.11\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.read_gpickle('./email_prediction.txt')\n",
    "print(nx.info(G))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "edges = pd.DataFrame(list(G.edges(data=True)))\n",
    "nodes = pd.DataFrame(list(G.nodes(data=True)))\n",
    "\n",
    "# Use a lambda to pull out the attributes from the attributes dictionary in column 1\n",
    "nodes['Department'] = nodes.loc[:, 1].map(lambda x: x['Department'])\n",
    "nodes['ManagementSalary'] = nodes.loc[:, 1].map(lambda x: x['ManagementSalary'])\n",
    "del nodes[1]\n",
    "\n",
    "edges.to_csv('email_edges.csv')\n",
    "nodes.to_csv('email_nodes.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import edges and nodes into networkx 2.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.read_csv('../_data/email_nodes.csv', index_col=0, \n",
    "                    names=['node', 'Department', 'ManagementSalary'])\n",
    "edges = pd.read_csv('../_data/email_edges.csv', index_col=0,\n",
    "                   names=['n1', 'n2', 'attr'])\n",
    "nodes.sample(3)\n",
    "edges.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create graph from edges, then add nodes\n",
    "G = nx.from_pandas_dataframe(edges, 'n1', 'n2', edge_attr='attr')\n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = [G.add_node(nodes.loc[n, 'node'], \n",
    "                Department=nodes.loc[n, 'Department'], \n",
    "                ManagementSalary=nodes.loc[n, 'ManagementSalary']) for n in nodes.index \n",
    "     if n in list(G.nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.nodes(data=True))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2A - Salary Prediction\n",
    "\n",
    "Using network `G`, identify the people in the network with missing values for the node attribute `ManagementSalary` and predict whether or not these individuals are receiving a management position salary.\n",
    "\n",
    "To accomplish this, you will need to create a matrix of node features using networkx, train a sklearn classifier on nodes that have `ManagementSalary` data, and predict a probability of the node receiving a management salary for nodes where `ManagementSalary` is missing.\n",
    "\n",
    "\n",
    "\n",
    "Your predictions will need to be given as the probability that the corresponding employee is receiving a management position salary.\n",
    "\n",
    "The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).\n",
    "\n",
    "Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).\n",
    "\n",
    "Using your trained classifier, return a series of length 252 with the data being the probability of receiving management salary, and the index being the node id.\n",
    "\n",
    "    Example:\n",
    "    \n",
    "        1       1.0\n",
    "        2       0.0\n",
    "        5       0.8\n",
    "        8       1.0\n",
    "            ...\n",
    "        996     0.7\n",
    "        1000    0.5\n",
    "        1001    0.0\n",
    "        Length: 252, dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=G.nodes())\n",
    "df['Department'] = pd.Series(nx.get_node_attributes(G, 'Department'))\n",
    "df['ManagementSalary'] = pd.Series(nx.get_node_attributes(G, 'ManagementSalary'))\n",
    "df['clustering'] = pd.Series(nx.clustering(G))\n",
    "df['degree_0'] = pd.Series([x[1] for x in G.degree()])\n",
    "df['degree_1'] = pd.Series([x[1] for x in G.degree()])\n",
    "df['degree_cent'] = pd.Series(nx.degree_centrality(G))\n",
    "df['closeness'] = pd.Series(nx.closeness_centrality(G))\n",
    "df['betweenness'] = pd.Series(nx.betweenness_centrality(G))\n",
    "df['pagerank'] = pd.Series(nx.pagerank(G, alpha=0.80))\n",
    "df['hub'] = pd.Series(nx.hits(G)[0])\n",
    "df['authority'] = pd.Series(nx.hits(G)[1])\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Department.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df.Department, prefix='dept').head()\n",
    "df = pd.concat([df, pd.get_dummies(df.Department, prefix='dept', drop_first=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Department']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in train test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train test\n",
    "df_test_mask = pd.isnull(df.loc[:, 'ManagementSalary'])\n",
    "df_train = df[~df_test_mask][:]  # [:] copies the slice\n",
    "df_test = df[df_test_mask][:]\n",
    "\n",
    "# Train set X, y\n",
    "y_train = df_train.pop('ManagementSalary').astype('f').astype('i')\n",
    "X_train = df_train\n",
    "idx_train = df_train.index\n",
    "\n",
    "# Test set X\n",
    "df_test.drop('ManagementSalary', axis=1, inplace=True)\n",
    "X_test = df_test\n",
    "idx_test = df_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()\n",
    "num_types = ['int','float', 'uint8']\n",
    "X_train.select_dtypes(num_types).sample()\n",
    "X_test.select_dtypes(num_types).sample()\n",
    "features = X_train.select_dtypes(num_types).columns\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train.select_dtypes(num_types))\n",
    "X_test_sc = scaler.transform(X_test.select_dtypes(num_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA transform X to principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Vectors - Eigen Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = ['V'+str(x) for x in range(1, n_components+1)]\n",
    "components = ['PC'+str(x) for x in range(1, n_components+1)]\n",
    "pca_loadings = pd.DataFrame(pca.components_.T, index=features, columns=vectors)\n",
    "pca_loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principle Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca = pd.DataFrame(pca.fit_transform(X_train_sc), index=idx_train, columns=components)\n",
    "X_train_pca.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pca = pd.DataFrame(pca.fit_transform(X_test_sc), index=idx_test, columns=components)\n",
    "X_test_pca.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing, selection and metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    GradientBoostingClassifier(random_state=0),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=0),\n",
    "    KNeighborsClassifier(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_scores(model, *args, k=5, threshold=0.50):\n",
    "    \"\"\"CV scores\"\"\"\n",
    "    X, y = args\n",
    "    try:\n",
    "        predictions = cross_val_predict(model, X, y, cv=k, n_jobs=-1)\n",
    "        pred_probas = (cross_val_predict(model, X, y, cv=k, method='predict_proba', n_jobs=-1)[:, 1] > threshold) * 1\n",
    "        print('AUC - Test predict  {:.2%}'.format(roc_auc_score(y, predictions)))\n",
    "        print('AUC - Test probabil {:.2%}'.format(roc_auc_score(y, pred_probas)))\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_predictions(X_train, y_train, X_test, classifiers):\n",
    "\n",
    "    for clf in classifiers:\n",
    "        print('-'*80)\n",
    "        print(clf)\n",
    "\n",
    "        # Training scores\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred_train = clf.predict(X_train)\n",
    "        print('AUC - Train pred    {:.2%}'.format(roc_auc_score(y_train, pred_train)))\n",
    "\n",
    "        # CV scores\n",
    "        auc_scores(clf, X_train, y_train)\n",
    "\n",
    "        try:\n",
    "            # predict_proba: probability per class(p, 1-p)\n",
    "            predicted = pd.DataFrame(clf.predict_proba(X_test), columns=clf.classes_)\n",
    "            predicted['idx'] = idx_test\n",
    "            predicted.set_index('idx', inplace=True)\n",
    "            predicted.drop(0.0, axis=1, inplace=True)\n",
    "            pred_series = predicted.loc[:, 1.0]  # pd.Series(predicted.values)\n",
    "            assert type(pred_series) == pd.Series, 'wtf: ' + str(type(pred_series))\n",
    "        except:\n",
    "            pred_series = None\n",
    "            continue\n",
    "\n",
    "    return pred_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and evaluate classifiers based on scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_predictions(X_train_sc, y_train, X_test_sc, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and evaluate classifiers based on PCA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_predictions(X_train_pca, y_train, X_test_pca, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune best classifier with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'learning_rate': [.05, .1, .2, .4, .8],\n",
    "    'max_depth': [3, 4, 5, 6]}\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(random_state=0), parameters)\n",
    "clf.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(clf.cv_results_.keys())\n",
    "np.mean(clf.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(**clf.best_params_, random_state=0).fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict probability on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gb.predict_proba(X_test_sc), columns=clf.classes_).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2B - New Connections Prediction\n",
    "\n",
    "For the last part of this assignment, you will predict future connections between employees of the network. The future connections information has been loaded into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates a future connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "# Import preprocessing, selection and metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../_data/Future_Connections.csv', index_col=0, converters={0: eval})\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.sample(1000)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Future Connection'].value_counts()\n",
    "pd.isnull(df.loc[:, 'Future Connection']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset = pd.isnull(df.loc[:, 'Future Connection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df.loc[~df_testset, 'Future Connection'].values.astype('i')\n",
    "# y_test = df.loc[df_testset, 'Future Connection'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:10]\n",
    "# y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build data set of network metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether or not these edges will have a future connection.\n",
    "\n",
    "To accomplish this, you will need to create a matrix of features for the edges found in `future_connections` using networkx, train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and predict a probability of the edge being a future connection for those edges in `future_connections` where `Future Connection` is missing.\n",
    "\n",
    "\n",
    "\n",
    "Your predictions will need to be given as the probability of the corresponding edge being a future connection.\n",
    "\n",
    "The evaluation metric for this assignment is the Area Under the ROC Curve (AUC).\n",
    "\n",
    "Your grade will be based on the AUC score computed for your classifier. A model which with an AUC of 0.88 or higher will receive full points, and with an AUC of 0.82 or higher will pass (get 80% of the full points).\n",
    "\n",
    "Using your trained classifier, return a series of length 122112 with the data being the probability of the edge being a future connection, and the index being the edge as represented by a tuple of nodes.\n",
    "\n",
    "    Example:\n",
    "    \n",
    "        (107, 348)    0.35\n",
    "        (542, 751)    0.40\n",
    "        (20, 426)     0.55\n",
    "        (50, 989)     0.35\n",
    "                  ...\n",
    "        (939, 940)    0.15\n",
    "        (555, 905)    0.35\n",
    "        (75, 101)     0.65\n",
    "        Length: 122112, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 1: Common Neighbors (intercept)\n",
    "# The number of common neighbors of nodes ùëã and ùëå\n",
    "L = [(e[0], e[1], len(list(nx.common_neighbors(G, e[0], e[1])))) for e in df.index]\n",
    "\n",
    "df['common_nb'] = [p for u, v, p in L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 1: Common Neighbors (intercept)\n",
    "# The number of common neighbors of nodes ùëã and ùëå\n",
    "# L = [(e[0], e[1], len(list(nx.common_neighbors(G, e[0], e[1]))))\n",
    "#      for e in nx.non_edges(G)]\n",
    "\n",
    "# df['pair'] = [(u, v) for u, v, p in L]\n",
    "# df['common_nb'] = [p for u, v, p in L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 2: Jaccard Coefficient (intercept over union)\n",
    "# Number of common neighbors normalized by the total number of neighbors\n",
    "# common_neighbors/total_neighbors\n",
    "df['jaccard'] = pd.Series([p for u, v, p in nx.jaccard_coefficient(G, df.index)]).values\n",
    "\n",
    "# Returns:\n",
    "# piter ‚Äì An iterator of 3-tuples in the form (u, v, p) \n",
    "# where (u, v) is a pair of nodes and p is their Jaccard coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 3: Resource\n",
    "# Fraction of a ‚Äùresource‚Äù that a node can send to another through their common neighbors\n",
    "# sum(1/degree_common_neighbor)\n",
    "df['resource'] = pd.Series([p for u, v, p in nx.resource_allocation_index(G, df.index)]).values\n",
    "\n",
    "# Returns:\n",
    "# piter ‚Äì An iterator of 3-tuples in the form (u, v, p) \n",
    "# where (u, v) is a pair of nodes and p is their resource allocation index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 4:\n",
    "# Adamic Adar Index\n",
    "# Similar to resource allocation index, but with log in the denominator\n",
    "# sum(1/log(degree_common_neighbor))\n",
    "df['adamic_adar'] = pd.Series([p for u, v, p in nx.adamic_adar_index(G, df.index)]).values\n",
    "\n",
    "# Returns: \n",
    "# piter ‚Äì An iterator of 3-tuples in the form (u, v, p) \n",
    "# where (u, v) is a pair of nodes and p is their Adamic-Adar index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5:\n",
    "# Preferential Attachment\n",
    "# In the preferential attachment model, nodes with high degree get more neighbors\n",
    "# degree_source * degree_target\n",
    "df['pref_att'] = pd.Series([p for u, v, p in nx.preferential_attachment(G, df.index)]).values\n",
    "\n",
    "# Returns:\n",
    "# piter ‚Äì An iterator of 3-tuples in the form (u, v, p) \n",
    "# where (u, v) is a pair of nodes and p is their preferential attachment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 6:\n",
    "# Community Common Neighbors\n",
    "# Number of common neighbors with bonus of 1 for each neighbor in same community\n",
    "# f(u) = 1 if same community else 0\n",
    "# sum(f(u) * degree)\n",
    "for i, dept in enumerate(nx.get_node_attributes(G, 'Department')):\n",
    "    G.node[i]['community'] = dept\n",
    "    \n",
    "df['com_common_nb'] = pd.Series([p for u, v, p in nx.cn_soundarajan_hopcroft(G, df.index)]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 7:\n",
    "# Community Resource Allocation\n",
    "# Similar to resource allocation index, but only considering nodes in the same community\n",
    "# f(u) = 1 if same community else 0\n",
    "# sum(f(u)/degree)\n",
    "df['com_resource'] = pd.Series([p for u, v, p in nx.ra_index_soundarajan_hopcroft(G, df.index)]).values\n",
    "\n",
    "# Returns:\n",
    "# piter ‚Äì An iterator of 3-tuples in the form (u, v, p) \n",
    "# where (u, v) is a pair of nodes and p is their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure 8: TODO\n",
    "# Community Resource Allocation\n",
    "# Similar to resource allocation index, but only considering nodes in the same community\n",
    "# f(u) = 1 if same community else 0\n",
    "# sum(f(u)/degree)\n",
    "df['cn_com_resource'] = pd.Series([p for u, v, p in nx.cn_soundarajan_hopcroft(G, df.index)]).values\n",
    "\n",
    "# Returns:\n",
    "# piter ‚Äì An iterator of 3-tuples in the form (u, v, p) \n",
    "# where (u, v) is a pair of nodes and p is their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[~df_testset][:]\n",
    "del X_train['Future Connection']\n",
    "\n",
    "X_test = df[df_testset][:]\n",
    "del X_test['Future Connection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.sample()\n",
    "X_test.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_types = ['int','float', 'uint8']\n",
    "X_train.select_dtypes(num_types).sample()\n",
    "X_test.select_dtypes(num_types).sample()\n",
    "features = X_train.select_dtypes(num_types).columns\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train.select_dtypes(num_types))\n",
    "X_test_sc = scaler.transform(X_test.select_dtypes(num_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(random_state=0),\n",
    "    GradientBoostingClassifier(random_state=0),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    AdaBoostClassifier(learning_rate=0.1, n_estimators=100, random_state=0),\n",
    "    KNeighborsClassifier(),\n",
    "#     LinearSVC(random_state=0)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_scores(model, *args, k=5, threshold=0.50):\n",
    "    \"\"\"CV scores\"\"\"\n",
    "    X, y = args\n",
    "    predictions = cross_val_predict(model, X, y, cv=k, n_jobs=-1)\n",
    "    print('AUC - Test predict  {:.2%}'.format(roc_auc_score(y, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_connections_predictions(X_train, y_train, X_test, classifiers):\n",
    "\n",
    "    for clf in classifiers:\n",
    "        print('-'*80)\n",
    "        print(clf)\n",
    "\n",
    "        # Training scores\n",
    "        clf.fit(X_train, y_train)\n",
    "        pred_train = clf.predict(X_train)\n",
    "        print('AUC - Train pred    {:.2%}'.format(roc_auc_score(y_train, pred_train)))\n",
    "\n",
    "        # CV scores\n",
    "        auc_scores(clf, X_train, y_train)\n",
    "\n",
    "        try:\n",
    "            # predict_proba: probability per class(p, 1-p)\n",
    "            predicted = pd.DataFrame(clf.predict_proba(X_test), columns=clf.classes_)\n",
    "            predicted['idx'] = idx_test\n",
    "            predicted.set_index('idx', inplace=True)\n",
    "            predicted.drop(0.0, axis=1, inplace=True)\n",
    "            pred_series = predicted.loc[:, 1.0]  # pd.Series(predicted.values)\n",
    "            assert type(pred_series) == pd.Series, 'wtf: ' + str(type(pred_series))\n",
    "        except:\n",
    "            pred_series = None\n",
    "            continue\n",
    "\n",
    "    return pred_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and evaluate classifiers based on scaled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_connections_predictions(X_train_sc, y_train, X_test_sc, classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and evaluate classifiers based on PCA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune best classifier with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'learning_rate': [.05, .1, .2, .4, .8],\n",
    "    'max_depth': [3, 4, 5, 6]}\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(random_state=0), parameters, return_train_score=True)\n",
    "clf.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(clf.cv_results_['mean_test_score']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best estimator and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train on best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(**clf.best_params_, random_state=0).fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 1 - gb.train_score_\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB(priors=None).fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((nb.predict(X_test_sc) == gb.predict(X_test_sc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.predict_proba(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame\n",
    "df_naive_bayes = pd.DataFrame(nb.predict_proba(X_test_sc), columns=clf.classes_)\n",
    "df_gradient_boost = pd.DataFrame(gb.predict_proba(X_test_sc), columns=clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
